#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
build.py - ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«ã‚µã‚¤ãƒˆé™çš„ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’è¡Œã„ã¾ã™ï¼š
1. Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
2. raw_images/ å†…ã®ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºã—ã¦ static/images/ ã«å‡ºåŠ›
3. Jinja2 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ HTML ã‚’ç”Ÿæˆ

Usage:
    python build.py [--csv-url URL]
"""

import os
import sys
import argparse
from pathlib import Path
from datetime import datetime
import json

import pandas as pd
import markdown
import requests
from jinja2 import Environment, FileSystemLoader
from PIL import Image

# =============================================================================
# è¨­å®š
# =============================================================================

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
BASE_DIR = Path(__file__).parent.resolve()

# å„ç¨®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
TEMPLATES_DIR = BASE_DIR / "templates"
CONTENT_DIR = BASE_DIR / "content"
DATA_DIR = BASE_DIR / "data"
RAW_IMAGES_DIR = BASE_DIR / "raw_images"
STATIC_DIR = BASE_DIR / "static"
OUTPUT_IMAGES_DIR = STATIC_DIR / "images"
PUBLIC_DIR = BASE_DIR / "public"
CONFIG_FILE = BASE_DIR / "config.json"
DOWNLOAD_HISTORY_FILE = DATA_DIR / ".download_history.json"

# ç”»åƒå‡¦ç†è¨­å®š
MAX_IMAGE_WIDTH = 1200  # æœ€å¤§å¹…ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰
MAX_IMAGE_HEIGHT = 800  # æœ€å¤§é«˜ã•ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰
IMAGE_QUALITY = 85      # JPEG/WebPå“è³ªï¼ˆ1-100ï¼‰
OUTPUT_FORMAT = "webp"  # å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆwebp ã¾ãŸã¯ jpgï¼‰

# Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®å…¬é–‹CSV URL
# ç’°å¢ƒå¤‰æ•° CSV_URL ã§è¨­å®šã™ã‚‹ã‹ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•° --csv-url ã§æŒ‡å®šã—ã¦ãã ã•ã„
DEFAULT_CSV_URL = os.environ.get("CSV_URL", "")

# å†™çœŸæŠ•ç¨¿ãƒ•ã‚©ãƒ¼ãƒ ç”¨ã®Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå…¬é–‹CSV URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
DEFAULT_PHOTO_URL = os.environ.get("PHOTO_URL", "")


# =============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# =============================================================================

def normalize_form_df(df: pd.DataFrame, kind: str) -> pd.DataFrame:
    """Googleãƒ•ã‚©ãƒ¼ãƒ ç”±æ¥ã®CSVã‚’å…±é€šã‚¹ã‚­ãƒ¼ãƒã«æ­£è¦åŒ–ã—ã¾ã™ã€‚

    å…±é€šã‚¹ã‚­ãƒ¼ãƒ:
      - timestamp: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
      - comment:   æƒ³ã„å‡ºæœ¬æ–‡
      - name:      å…¬é–‹å¯èƒ½ãªãŠåå‰
      - menu:      å¥½ãã ã£ãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®ã¿æƒ³å®šï¼‰
      - photo:     å†™çœŸURLï¼ˆå†™çœŸãƒ•ã‚©ãƒ¼ãƒ ã®ã¿æƒ³å®šï¼‰

    kind:
      - "comments": ã‚³ãƒ¡ãƒ³ãƒˆæŠ•ç¨¿ãƒ•ã‚©ãƒ¼ãƒ 
      - "photos":   å†™çœŸæŠ•ç¨¿ãƒ•ã‚©ãƒ¼ãƒ 
    """
    if df is None or df.empty:
        return pd.DataFrame(columns=["timestamp", "comment", "name", "menu", "photo"])

    cols = [str(c) for c in df.columns]

    def find_col(*keywords: str) -> str | None:
        for c in cols:
            for kw in keywords:
                if kw and kw in c:
                    return c
        return None

    # ã§ãã‚‹ã ã‘åˆ—åã§æ‹¾ã†ï¼ˆæ—¥æœ¬èª/è‹±èªã®æºã‚Œã«è€ãˆã‚‹ï¼‰
    ts_col = find_col("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—")
    name_col = find_col("å…¬é–‹å¯èƒ½ãªãŠåå‰ï¼ˆãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã€ä»»æ„ï¼‰")

    if kind == "comments":
        # ã‚³ãƒ¡ãƒ³ãƒˆæœ¬æ–‡ã¯ã€Œæƒ³ã„å‡º/æ€ã„å‡ºã€ç³»ã‚’å„ªå…ˆã€‚æ±ç”¨ã® "comment" ã‚‚è¨±å®¹ã€‚
        comment_col = find_col(
            "æƒ³ã„å‡ºï¼ˆå¿…é ˆï¼‰"
        )
        menu_col = find_col("å¥½ãã ã£ãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼ˆè¤‡æ•°å¯ã€ä»»æ„ï¼‰")
        # ã‚³ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒ å´ã«å†™çœŸåˆ—ãŒæ··ã–ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ãŸã‚æ‹¾ã£ã¦ãŠã
        photo_col = find_col("æƒ³ã„å‡ºã®å†™çœŸ")

    else:
        # photos
        comment_col = find_col(
            "å†™çœŸã«ã¾ã¤ã‚ã‚‹æƒ³ã„å‡º"
        )
        photo_col = "æƒ³ã„å‡ºã®å†™çœŸ"
        if photo_col not in df.columns:
            raise KeyError(f"PHOTO_URL åˆ— '{photo_col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚columns={cols}")
        menu_col = find_col("å¥½ãã ã£ãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼", "ãƒ¡ãƒ‹ãƒ¥ãƒ¼", "menu")  # ã‚‚ã—æ··ã–ã£ã¦ã¦ã‚‚æ‹¾ã†
    # åˆ—åãŒå–ã‚Œãªã„å ´åˆã¯ã€å¾“æ¥ã®ä¸¦ã³ï¼ˆå…ˆé ­ã‹ã‚‰ï¼‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    def safe_iloc(i: int) -> pd.Series:
        if df.shape[1] > i:
            return df.iloc[:, i]
        return pd.Series([""] * len(df))

    out = pd.DataFrame({
        "timestamp": df[ts_col] if ts_col else safe_iloc(0),
        "comment": df[comment_col] if comment_col else safe_iloc(1),
        "name": df[name_col] if name_col else safe_iloc(2),
        "menu": df[menu_col] if menu_col else pd.Series([""] * len(df)),
        "photo": df[photo_col] if photo_col else pd.Series([""] * len(df)),
    })

    # NaNã‚’ç©ºæ–‡å­—ã«å¯„ã›ã‚‹ï¼ˆå¾Œæ®µã®å‡¦ç†ã‚’å˜ç´”åŒ–ï¼‰
    out = out.fillna("")
    return out

def ensure_directories():
    """
    å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€ãªã‘ã‚Œã°ä½œæˆã—ã¾ã™ã€‚
    """
    directories = [DATA_DIR, OUTPUT_IMAGES_DIR, PUBLIC_DIR]
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
        print(f"âœ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª: {directory}")


def load_config() -> dict:
    """
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆconfig.jsonï¼‰ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
    
    Returns:
        dict: è¨­å®šæƒ…å ±ã®è¾æ›¸
    """
    if not CONFIG_FILE.exists():
        print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {CONFIG_FILE}")
        print("   ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
        return {}
    
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config
    except Exception as e:
        print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        print("   ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
        return {}


def fetch_csv_data(csv_url: str) -> pd.DataFrame:
    """
    Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        csv_url: CSVå½¢å¼ã§å…¬é–‹ã•ã‚ŒãŸã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URL
    
    Returns:
        pandas.DataFrame: å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿
    """
    print(f"\nğŸ“¥ CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­: {csv_url[:50]}...")
    
    try:
        # URLã‹ã‚‰CSVã‚’å–å¾—
        response = requests.get(csv_url, timeout=30)
        response.raise_for_status()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’UTF-8ã«è¨­å®š
        response.encoding = 'utf-8'
        
        # CSVã‚’DataFrameã«å¤‰æ›
        from io import StringIO
        df = pd.read_csv(StringIO(response.text), encoding='utf-8')
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ä¿å­˜
        cache_path = DATA_DIR / "comments.csv"
        df.to_csv(cache_path, index=False, encoding="utf-8")
        print(f"âœ“ CSVãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {cache_path}")
        print(f"  â†’ {len(df)} ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã¾ã—ãŸ")
        print(f"df(Comments):\n{df}")
        
        return df
        
    except requests.RequestException as e:
        print(f"âš ï¸ CSVã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ä½¿ç”¨
        cache_path = DATA_DIR / "comments.csv"
        if cache_path.exists():
            print(f"  â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™: {cache_path}")
            return pd.read_csv(cache_path, encoding='utf-8')
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚ãªã‘ã‚Œã°ç©ºã®DataFrameã‚’è¿”ã™
        print("  â†’ ç©ºã®DataFrameã‚’ä½¿ç”¨ã—ã¾ã™")
        return pd.DataFrame()


def load_local_csv() -> pd.DataFrame:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥CSVã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
    æ­£è¦åŒ–æ¸ˆã¿ã®merged.csvã‚’å„ªå…ˆçš„ã«èª­ã¿è¾¼ã¿ã¾ã™ã€‚
    
    Returns:
        pandas.DataFrame: èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ç©ºã®DataFrameï¼‰
    """
    # æ­£è¦åŒ–æ¸ˆã¿ã®merged.csvã‚’å„ªå…ˆ
    merged_path = DATA_DIR / "merged.csv"
    if merged_path.exists():
        return pd.read_csv(merged_path, encoding='utf-8')
    
    # ãªã‘ã‚Œã°comments.csvã‚’èª­ã¿è¾¼ã‚“ã§æ­£è¦åŒ–
    cache_path = DATA_DIR / "comments.csv"
    if cache_path.exists():
        df = pd.read_csv(cache_path, encoding='utf-8')
        # æ­£è¦åŒ–ã—ã¦è¿”ã™
        return normalize_form_df(df, "comments")
    
    return pd.DataFrame()


def fetch_and_merge_csv_data(csv_url: str, photo_url: str = "") -> pd.DataFrame:
    """
    ã‚³ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒ ã¨å†™çœŸãƒ•ã‚©ãƒ¼ãƒ ã®ä¸¡æ–¹ã®CSVã‚’å–å¾—ã—ã¦ãƒãƒ¼ã‚¸ã—ã¾ã™ã€‚
    
    Args:
        csv_url: ã‚³ãƒ¡ãƒ³ãƒˆæŠ•ç¨¿ãƒ•ã‚©ãƒ¼ãƒ ã®CSV URL
        photo_url: å†™çœŸæŠ•ç¨¿ãƒ•ã‚©ãƒ¼ãƒ ç”¨ã®CSV URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        pandas.DataFrame: ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    """
    # ã‚³ãƒ¡ãƒ³ãƒˆCSVã‚’å–å¾—
    df_comments = fetch_csv_data(csv_url)
    # ã‚³ãƒ¡ãƒ³ãƒˆCSVã‚’å…±é€šã‚¹ã‚­ãƒ¼ãƒã«æ­£è¦åŒ–
    df_comments_norm = normalize_form_df(df_comments, "comments")
    
    # å†™çœŸæŠ•ç¨¿ãƒ•ã‚©ãƒ¼ãƒ ã®CSVã‚‚å–å¾—ï¼ˆURLãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    if photo_url and photo_url.strip():
        print(f"\nğŸ“¥ å†™çœŸæŠ•ç¨¿ãƒ•ã‚©ãƒ¼ãƒ ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
        try:
            response = requests.get(photo_url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            from io import StringIO
            df_photos = pd.read_csv(StringIO(response.text), encoding='utf-8')
            
            # å†™çœŸæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            photo_cache_path = DATA_DIR / "photos.csv"
            df_photos.to_csv(photo_cache_path, index=False, encoding="utf-8")
            print(f"âœ“ å†™çœŸæŠ•ç¨¿CSVã‚’ä¿å­˜: {photo_cache_path}")
            print(f"  â†’ {len(df_photos)} ä»¶ã®å†™çœŸæŠ•ç¨¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
            print(f"df(Photos):\n{df_photos}")
            
            # å†™çœŸæŠ•ç¨¿CSVã‚’å…±é€šã‚¹ã‚­ãƒ¼ãƒã«æ­£è¦åŒ–
            df_photos_norm = normalize_form_df(df_photos, "photos")
            # ä¸¡æ–¹ã®DataFrameã‚’ãƒãƒ¼ã‚¸ï¼ˆåˆ—åãŒåŒã˜ã‚‚ã®ã¯åŒã˜åˆ—ã«ã€ç‰‡æ–¹ã«ãªã„åˆ—ã¯ç©ºæ–‡å­—ï¼‰
            if not df_photos_norm.empty:
                df_merged = pd.concat([df_comments_norm, df_photos_norm], ignore_index=True, sort=False)
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§æ–°ã—ã„ã‚‚ã®ãŒå…ˆé ­ã«æ¥ã‚‹ã‚ˆã†ã‚½ãƒ¼ãƒˆ
                df_merged["_ts"] = pd.to_datetime(df_merged["timestamp"], errors="coerce")
                df_merged = df_merged.sort_values("_ts", ascending=False).drop(columns=["_ts"])
                # df_merged = df_merged.sort_values("_ts", ascending=False)
                print(f"âœ“ ã‚³ãƒ¡ãƒ³ãƒˆã¨å†™çœŸæŠ•ç¨¿ã‚’ãƒãƒ¼ã‚¸: åˆè¨ˆ {len(df_merged)} ä»¶")
                print("df_merged:\n", df_merged)
                df_merged.to_csv(DATA_DIR / "merged.csv", index=False, encoding="utf-8")
                return df_merged
                
        except requests.RequestException as e:
            print(f"âš ï¸ å†™çœŸæŠ•ç¨¿CSVã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print(f"  â†’ ã‚³ãƒ¡ãƒ³ãƒˆã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    # ã‚³ãƒ¡ãƒ³ãƒˆã®ã¿ã®å ´åˆã‚‚ã‚½ãƒ¼ãƒˆã—ã¦è¿”ã™
    df_comments_norm["_ts"] = pd.to_datetime(df_comments_norm["timestamp"], errors="coerce")
    df_comments_norm = df_comments_norm.sort_values("_ts", ascending=False).drop(columns=["_ts"])
    return df_comments_norm


def download_image_from_google_drive(url: str, output_path: Path) -> bool:
    """Google Driveã®URLï¼ˆã¾ãŸã¯ç›´æ¥ç”»åƒURLï¼‰ã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

    æ³¨æ„:
      - Driveã®å…±æœ‰è¨­å®šãŒã€Œãƒªãƒ³ã‚¯ã‚’çŸ¥ã£ã¦ã„ã‚‹å…¨å“¡ã€ç­‰ã§å…¬é–‹ã•ã‚Œã¦ã„ãªã„å ´åˆã¯å–å¾—ã§ãã¾ã›ã‚“ã€‚
      - å¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«ç­‰ã§ Google ã®ã‚¦ã‚¤ãƒ«ã‚¹ã‚¹ã‚­ãƒ£ãƒ³ç¢ºèªï¼ˆconfirm=...ï¼‰ãŒå¿…è¦ãªå ´åˆã¯2æ®µéšã§å–å¾—ã—ã¾ã™ã€‚
    """
    try:
        url = str(url).strip()
        if not url:
            return False

        # 1) ç›´æ¥ç”»åƒURLï¼ˆgoogleusercontentç­‰ï¼‰ã¯ãã®ã¾ã¾GET
        if "drive.google.com" not in url:
            resp = requests.get(url, timeout=30)
            resp.raise_for_status()
            ct = (resp.headers.get("Content-Type") or "").lower()
            if "text/html" in ct:
                print(f"  âœ— ç”»åƒã§ã¯ãªãHTMLãŒè¿”ã‚Šã¾ã—ãŸï¼ˆã‚¢ã‚¯ã‚»ã‚¹æ¨©/URLã‚’ç¢ºèªï¼‰: {url}")
                return False
            with open(output_path, "wb") as f:
                f.write(resp.content)
            return True

        # 2) Google Drive URL ã‹ã‚‰ file_id ã‚’æŠ½å‡º
        file_id = None
        if "id=" in url:
            file_id = url.split("id=")[1].split("&")[0]
        elif "/d/" in url:
            file_id = url.split("/d/")[1].split("/")[0]
        elif "/file/d/" in url:
            file_id = url.split("/file/d/")[1].split("/")[0]
        elif "/uc?" in url and "id=" in url:
            file_id = url.split("id=")[1].split("&")[0]

        if not file_id:
            print(f"  âœ— URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {url}")
            return False

        session = requests.Session()

        def _get_confirm_token(r: requests.Response) -> str | None:
            # cookie ã« confirm ãŒä»˜ãã“ã¨ãŒã‚ã‚‹
            for k, v in r.cookies.items():
                if k.startswith("download_warning") and v:
                    return v
            # HTMLå†…ã« confirm= ãŒåŸ‹ã‚è¾¼ã¾ã‚Œã‚‹ã‚±ãƒ¼ã‚¹
            import re
            m = re.search(r"confirm=([0-9A-Za-z_]+)", r.text or "")
            if m:
                return m.group(1)
            return None

        # ã¾ãšã¯é€šå¸¸ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLã¸
        download_url = f"https://drive.google.com/uc?export=download&id={file_id}"
        r = session.get(download_url, timeout=30)
        r.raise_for_status()

        ct = (r.headers.get("Content-Type") or "").lower()

        # confirm ãŒå¿…è¦ãªå ´åˆï¼ˆã‚¦ã‚¤ãƒ«ã‚¹ã‚¹ã‚­ãƒ£ãƒ³/ã‚µã‚¤ã‚ºç­‰ï¼‰
        if "text/html" in ct:
            token = _get_confirm_token(r)
            if token:
                r = session.get(download_url + f"&confirm={token}", timeout=30)
                r.raise_for_status()
                ct = (r.headers.get("Content-Type") or "").lower()

        # ãã‚Œã§ã‚‚HTMLãªã‚‰ã€æ¨©é™ä¸è¶³ or ãƒ­ã‚°ã‚¤ãƒ³å¿…é ˆ
        if "text/html" in ct:
            print(f"  âœ— Driveã‹ã‚‰ç”»åƒã‚’å–å¾—ã§ãã¾ã›ã‚“ï¼ˆå…±æœ‰è¨­å®š/ãƒ­ã‚°ã‚¤ãƒ³å¿…é ˆã®å¯èƒ½æ€§ï¼‰: {url}")
            return False

        with open(output_path, "wb") as f:
            f.write(r.content)

        return True

    except requests.RequestException as e:
        print(f"  âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—(HTTP): {e}")
        return False
    except Exception as e:
        print(f"  âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return False


def load_download_history() -> dict:
    """
    ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
    
    Returns:
        dict: URLãƒãƒƒã‚·ãƒ¥ã‚’ã‚­ãƒ¼ã¨ã—ãŸå±¥æ­´è¾æ›¸
    """
    if DOWNLOAD_HISTORY_FILE.exists():
        try:
            with open(DOWNLOAD_HISTORY_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"  âš ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    return {}


def save_download_history(history: dict):
    """
    ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’ä¿å­˜ã—ã¾ã™ã€‚
    
    Args:
        history: URLãƒãƒƒã‚·ãƒ¥ã‚’ã‚­ãƒ¼ã¨ã—ãŸå±¥æ­´è¾æ›¸
    """
    try:
        with open(DOWNLOAD_HISTORY_FILE, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"  âš ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã®ä¿å­˜ã«å¤±æ•—: {e}")


def download_images_from_csv(df: pd.DataFrame) -> int:
    """
    CSVã«å«ã¾ã‚Œã‚‹Google Drive URLã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    
    Args:
        df: ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®DataFrame
    
    Returns:
        int: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸç”»åƒã®æ•°
    """
    import hashlib
    
    if df.empty:
        return 0
    
    print(f"\nğŸ“¥ CSVå†…ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    # å†™çœŸURLã®ã‚«ãƒ©ãƒ ã‚’æ¢ã™ï¼ˆæ­£è¦åŒ–å¾Œã¯ photo ã‚’å„ªå…ˆï¼‰
    if "photo" in df.columns:
        photo_col_idx = list(df.columns).index("photo")
    else:
        photo_col_idx = None
        photo_col_names = ['å†™çœŸ', 'photo', 'Photo', 'ç”»åƒ', 'image', 'Image']
        for idx, col_name in enumerate(df.columns):
            if any(keyword in str(col_name) for keyword in photo_col_names):
                photo_col_idx = idx
                break
    
    # å†™çœŸåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    if photo_col_idx is None:
        print(f"  â„¹ï¸ CSVã«å†™çœŸåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return 0
    
    RAW_IMAGES_DIR.mkdir(parents=True, exist_ok=True)
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’èª­ã¿è¾¼ã¿
    download_history = load_download_history()
    downloaded_count = 0
    history_updated = False
    
    for idx, row in df.iterrows():
        # å†™çœŸURLã®ã‚«ãƒ©ãƒ ã«ã‚¢ã‚¯ã‚»ã‚¹
        if len(row) <= photo_col_idx:
            continue

        photo_url = row.iloc[photo_col_idx]

        # URLãŒç©ºã§ãªã„å ´åˆã§ã€å®Ÿéš›ã«URLã‚‰ã—ã„æ–‡å­—åˆ—ã®å ´åˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if pd.notna(photo_url) and str(photo_url).strip() and str(photo_url) != "nan":
            photo_url_str = str(photo_url).strip()
            if not (photo_url_str.startswith('http') or 'drive.google.com' in photo_url_str):
                # å…±æœ‰URLã§ã¯ãªã„å€¤ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åãªã©ï¼‰ã®å¯èƒ½æ€§
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— + URLã®ãƒãƒƒã‚·ãƒ¥ï¼‰
            # URLã®ãƒãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€åŒã˜å†™çœŸã¯å¸¸ã«åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«åã«ãªã‚‹
            timestamp = row.get("timestamp", "") if "timestamp" in df.columns else (row.iloc[0] if len(row) > 0 else "")
            safe_timestamp = str(timestamp).replace("/", "").replace(":", "").replace(" ", "_")
            
            # URLã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆæœ€åˆã®8æ–‡å­—ã®ã¿ä½¿ç”¨ï¼‰
            url_hash = hashlib.md5(photo_url_str.encode('utf-8')).hexdigest()[:8]
            filename = f"photo_{safe_timestamp}_{url_hash}.jpg"
            output_path = RAW_IMAGES_DIR / filename

            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã§ãƒã‚§ãƒƒã‚¯
            if url_hash in download_history:
                # å±¥æ­´ã«ã‚ã‚‹ãŒã€ãƒ•ã‚¡ã‚¤ãƒ«ãŒå‰Šé™¤ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                if output_path.exists():
                    print(f"  âŠ™ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå±¥æ­´ã‚ã‚Šï¼‰: {filename}")
                    continue
                else:
                    print(f"  â„¹ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {filename}")
            
            # æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
            if output_path.exists():
                print(f"  âŠ™ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ï¼‰: {filename}")
                # å±¥æ­´ã«è¿½åŠ 
                download_history[url_hash] = {
                    "url": photo_url_str,
                    "filename": filename,
                    "timestamp": safe_timestamp,
                    "downloaded_at": datetime.now().isoformat()
                }
                history_updated = True
                continue

            print(f"  â¬‡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}")
            if download_image_from_google_drive(str(photo_url), output_path):
                print(f"  âœ“ ä¿å­˜å®Œäº†: {filename}")
                downloaded_count += 1
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã«è¿½åŠ 
                download_history[url_hash] = {
                    "url": photo_url_str,
                    "filename": filename,
                    "timestamp": safe_timestamp,
                    "downloaded_at": datetime.now().isoformat()
                }
                history_updated = True
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’ä¿å­˜ï¼ˆæ›´æ–°ãŒã‚ã£ãŸå ´åˆã®ã¿ï¼‰
    if history_updated:
        save_download_history(download_history)
        print(f"  âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’æ›´æ–°ã—ã¾ã—ãŸ")
    
    if downloaded_count > 0:
        print(f"  â†’ {downloaded_count} ä»¶ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
    else:
        print(f"  â†’ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–°ã—ã„ç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return downloaded_count


def process_images() -> list:
    """
    raw_images/ å†…ã®ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºã—ã¦ static/images/ ã«å‡ºåŠ›ã—ã¾ã™ã€‚
    
    Returns:
        list: å‡¦ç†ã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
    """
    print(f"\nğŸ–¼ï¸ ç”»åƒã‚’å‡¦ç†ä¸­...")
    
    # å¯¾å¿œã™ã‚‹ç”»åƒå½¢å¼
    supported_extensions = {".jpg", ".jpeg", ".png", ".gif", ".webp", ".bmp"}
    processed_images = []
    
    if not RAW_IMAGES_DIR.exists():
        print(f"  â†’ raw_images/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return processed_images
    
    # raw_images/ å†…ã®ã™ã¹ã¦ã®ç”»åƒã‚’å‡¦ç†
    for image_path in RAW_IMAGES_DIR.iterdir():
        if image_path.suffix.lower() not in supported_extensions:
            continue
        
        try:
            # ç”»åƒã‚’é–‹ã
            with Image.open(image_path) as img:
                # RGBAã®å ´åˆã¯RGBã«å¤‰æ›ï¼ˆWebP/JPEGç”¨ï¼‰
                if img.mode in ("RGBA", "P"):
                    img = img.convert("RGB")
                
                # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¶­æŒã—ã¦ãƒªã‚µã‚¤ã‚º
                img.thumbnail((MAX_IMAGE_WIDTH, MAX_IMAGE_HEIGHT), Image.Resampling.LANCZOS)
                
                # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
                output_filename = f"{image_path.stem}.{OUTPUT_FORMAT}"
                output_path = OUTPUT_IMAGES_DIR / output_filename
                
                # ä¿å­˜
                if OUTPUT_FORMAT == "webp":
                    img.save(output_path, "WEBP", quality=IMAGE_QUALITY)
                else:
                    img.save(output_path, "JPEG", quality=IMAGE_QUALITY)
                
                processed_images.append(output_filename)
                print(f"  âœ“ {image_path.name} â†’ {output_filename}")
                
        except Exception as e:
            print(f"  âœ— {image_path.name} ã®å‡¦ç†ã«å¤±æ•—: {e}")
    
    print(f"  â†’ {len(processed_images)} ä»¶ã®ç”»åƒã‚’å‡¦ç†ã—ã¾ã—ãŸ")
    return sorted(processed_images)


def load_markdown_content(filename: str) -> str:
    """
    Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§HTMLã«å¤‰æ›ã—ã¾ã™ã€‚
    
    Args:
        filename: content/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«å
    
    Returns:
        str: HTMLã«å¤‰æ›ã•ã‚ŒãŸå†…å®¹
    """
    filepath = CONTENT_DIR / filename
    
    if not filepath.exists():
        print(f"âš ï¸ {filepath} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return ""
    
    with open(filepath, "r", encoding="utf-8") as f:
        md_content = f.read()
    
    # Markdownã‚’HTMLã«å¤‰æ›ï¼ˆæ‹¡å¼µæ©Ÿèƒ½ä»˜ãï¼‰
    html_content = markdown.markdown(
        md_content,
        extensions=["extra", "nl2br", "sane_lists"]
    )
    
    return html_content


def extract_store_history(about_html: str) -> tuple:
    """
    åº—èˆ—ã®å¤‰é·æƒ…å ±ã‚’HTMLã‹ã‚‰æŠ½å‡ºã—ã¾ã™ã€‚
    
    Args:
        about_html: about.mdã‹ã‚‰å¤‰æ›ã•ã‚ŒãŸHTML
    
    Returns:
        tuple: (åº—èˆ—å¤‰é·ã‚’é™¤ã„ãŸHTML, åº—èˆ—å¤‰é·ã®ãƒªã‚¹ãƒˆ)
    """
    import re
    
    # ã€Œåº—èˆ—ã®å¤‰é·ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
    history_pattern = r'<h2>åº—èˆ—ã®å¤‰é·</h2>(.*?)<hr\s*/>'
    match = re.search(history_pattern, about_html, re.DOTALL)
    
    if not match:
        return about_html, []
    
    history_section = match.group(1)
    about_without_history = about_html.replace(match.group(0), '<hr />')
    
    # å„åº—èˆ—ã®æƒ…å ±ã‚’æŠ½å‡ºï¼ˆh3ã¨imgã‚’åˆ¥ã€…ã«ï¼‰
    stores = []
    
    # h3ã‚¿ã‚°ã‚’æ¢ã™
    h3_pattern = r'<h3>(.*?)</h3>'
    img_pattern = r'<img\s+alt="(.*?)"\s+src="(.*?)"\s*/>'
    
    h3_matches = list(re.finditer(h3_pattern, history_section))
    img_matches = list(re.finditer(img_pattern, history_section))
    
    # h3ã¨imgã‚’é †ç•ªã«ãƒãƒƒãƒãƒ³ã‚°
    for idx, (h3_match, img_match) in enumerate(zip(h3_matches, img_matches)):
        title = h3_match.group(1)
        alt_text = img_match.group(1)
        image_url = img_match.group(2)
        
        stores.append({
            'id': f'store{idx}',
            'title': title,
            'alt': alt_text,
            'image': image_url
        })
    
    return about_without_history, stores


def aggregate_menu_items(df: pd.DataFrame) -> dict:
    """
    ã€Œå¥½ãã ã£ãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€ã‚’é›†è¨ˆã—ã¾ã™ã€‚
    ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€å…¨è§’ã‚«ãƒ³ãƒã€ãã®ä»–ã®åŒºåˆ‡ã‚Šæ–‡å­—ã«å¯¾å¿œã—ã¾ã™ã€‚
    
    Args:
        df: ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®DataFrame
    
    Returns:
        dict: ãƒ¡ãƒ‹ãƒ¥ãƒ¼åã‚’ã‚­ãƒ¼ã€å‡ºç¾å›æ•°ã‚’å€¤ã¨ã—ãŸè¾æ›¸ï¼ˆé™é †ã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
    """
    import re
    from collections import Counter
    
    menu_counter = Counter()
    
    if df.empty:
        return {}
    
    # æ­£è¦åŒ–æ¸ˆã¿ã‚¹ã‚­ãƒ¼ãƒãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆ
    has_normalized = all(c in df.columns for c in ["timestamp", "comment", "name", "menu", "photo"])
    
    for idx, row in df.iterrows():
        if has_normalized:
            menu = row.get("menu", "")
        else:
            # æ—§æ¥ã®åˆ—ä¸¦ã³ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            menu = row.iloc[3] if len(row) > 3 else ""
        
        # ç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if pd.isna(menu) or str(menu).strip() == "" or str(menu) == "nan":
            continue
        
        menu_str = str(menu).strip()
        
        # è¤‡æ•°ã®åŒºåˆ‡ã‚Šæ–‡å­—ã«å¯¾å¿œ: ã‚«ãƒ³ãƒã€å…¨è§’ã‚«ãƒ³ãƒã€ã‚»ãƒŸã‚³ãƒ­ãƒ³ãªã©
        # æ­£è¦è¡¨ç¾ã§è¤‡æ•°ã®åŒºåˆ‡ã‚Šæ–‡å­—ã‚’ä¸€åº¦ã«åˆ†å‰²
        # ã‚«ãƒ³ãƒï¼ˆ,ï¼‰ã€å…¨è§’ã‚«ãƒ³ãƒï¼ˆã€ï¼‰ã€å…¨è§’ã‚«ãƒ³ãƒï¼ˆï¼Œï¼‰ã€ã‚»ãƒŸã‚³ãƒ­ãƒ³ï¼ˆ;ï¼‰ã€æ”¹è¡Œï¼ˆ\nï¼‰ãªã©ã§åˆ†å‰²
        # åŒºåˆ‡ã‚Šæ–‡å­—ã®å‰å¾Œã®ç©ºç™½ã‚‚å‰Šé™¤
        menu_items = re.split(r'[,ã€ï¼Œ;ï¼›\n]+', menu_str)
        
        # å„ãƒ¡ãƒ‹ãƒ¥ãƒ¼é …ç›®ã‚’æ­£è¦åŒ–ï¼ˆå‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤ã€ç©ºæ–‡å­—åˆ—ã‚’é™¤å¤–ï¼‰
        for item in menu_items:
            normalized = item.strip()
            # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚„é€šå¸¸ã®ã‚¹ãƒšãƒ¼ã‚¹ã®ã¿ã®é …ç›®ã¯é™¤å¤–
            if normalized and normalized not in ['', ' ', 'ã€€']:
                menu_counter[normalized] += 1
    
    # å‡ºç¾å›æ•°ã§é™é †ã‚½ãƒ¼ãƒˆã—ã¦è¾æ›¸ã«å¤‰æ›
    sorted_menus = dict(sorted(menu_counter.items(), key=lambda x: x[1], reverse=True))
    
    return sorted_menus


def prepare_comments_data(df: pd.DataFrame) -> list:
    """
    DataFrameã‚’ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”¨ã®è¾æ›¸ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚
    
    Args:
        df: ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®DataFrame
    
    Returns:
        list: ã‚³ãƒ¡ãƒ³ãƒˆã®è¾æ›¸ãƒªã‚¹ãƒˆ
    """
    comments = []
    
    # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    if df.empty:
        return comments
    
    # æ­£è¦åŒ–æ¸ˆã¿ã‚¹ã‚­ãƒ¼ãƒãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆ
    has_normalized = all(c in df.columns for c in ["timestamp", "comment", "name", "menu", "photo"])

    for idx, row in df.iterrows():
        if has_normalized:
            timestamp = row.get("timestamp", "")
            content = row.get("comment", "")
            name = row.get("name", "")
            menu = row.get("menu", "")
            photo_url = row.get("photo", "")
            ts_dt = pd.to_datetime(timestamp, errors="coerce")
        else:
            # æ—§æ¥ã®åˆ—ä¸¦ã³ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
            timestamp = row.iloc[0] if len(row) > 0 else ""
            content = row.iloc[1] if len(row) > 1 else ""
            name = row.iloc[2] if len(row) > 2 else "åŒ¿å"
            menu = row.iloc[3] if len(row) > 3 else ""
            ts_dt = pd.to_datetime(timestamp, errors="coerce")

            # å†™çœŸURLã®ã‚«ãƒ©ãƒ ã‚’æ¢ã™
            photo_url = ""
            photo_col_idx = None
            photo_col_names = ['å†™çœŸ', 'photo', 'Photo', 'ç”»åƒ', 'image', 'Image']
            for c_idx, col_name in enumerate(df.columns):
                if any(keyword in str(col_name) for keyword in photo_col_names):
                    photo_col_idx = c_idx
                    break
            if photo_col_idx is not None and len(row) > photo_col_idx:
                photo_url = row.iloc[photo_col_idx]

        # å†™çœŸã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ç‰¹å®šï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ç”»åƒï¼‰
        photo_filename = None
        if pd.notna(photo_url) and str(photo_url).strip() and str(photo_url) != "nan":
            import hashlib
            photo_url_str = str(photo_url).strip()
            # URLã‚‰ã—ã„æ–‡å­—åˆ—ã®å ´åˆã®ã¿ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            if photo_url_str.startswith('http') or 'drive.google.com' in photo_url_str:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— + URLã®ãƒãƒƒã‚·ãƒ¥ã§ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
                safe_timestamp = str(timestamp).replace("/", "").replace(":", "").replace(" ", "_")
                url_hash = hashlib.md5(photo_url_str.encode('utf-8')).hexdigest()[:8]
                photo_filename = f"photo_{safe_timestamp}_{url_hash}.webp"
        
        comment = {
            "timestamp": timestamp,
            "_ts_dt": ts_dt,
            "content": str(content),
            "menu": str(menu),
            "photo_url": str(photo_url),
            "photo_filename": photo_filename,  # ãƒ­ãƒ¼ã‚«ãƒ«ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
            "name": str(name)
        }
        
        # åå‰ãŒç©ºã®å ´åˆã¯ã€ŒåŒ¿åã€ã«
        if pd.isna(comment["name"]) or comment["name"].strip() == "" or comment["name"] == "nan":
            comment["name"] = "åŒ¿å"
        
        # ãƒ¡ãƒ‹ãƒ¥ãƒ¼æƒ…å ±ã‚’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«è¿½åŠ 
        if comment.get("menu") and not pd.isna(comment["menu"]) and comment["menu"].strip() and comment["menu"] != "nan":
            menu_text = f"\n\nã€å¥½ãã ã£ãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€‘\n{comment['menu']}"
            comment["content"] = comment["content"] + menu_text
        
        # ã‚³ãƒ¡ãƒ³ãƒˆãŒç©ºã§ãªã„å ´åˆã®ã¿è¿½åŠ 
        if not pd.isna(comment["content"]) and comment["content"].strip() and comment["content"] != "nan":
            comments.append(comment)
    
    # æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆï¼ˆTimestampãŒã‚ã‚‹å ´åˆï¼‰
    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰ã€‚ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯ãã®ã¾ã¾ã€‚
    try:
        comments.sort(key=lambda x: (x.get("_ts_dt") or pd.Timestamp.min), reverse=True)
    except Exception:
        pass

    # ã‚½ãƒ¼ãƒˆç”¨ã®ä¸€æ™‚ã‚­ãƒ¼ã‚’å‰Šé™¤
    for c in comments:
        c.pop("_ts_dt", None)
    
    return comments


def generate_html(comments: list, images: list, about_html: str, config: dict, store_history: list = None, menu_stats: dict = None):
    """
    Jinja2ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦HTMLã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    
    Args:
        comments: ã‚³ãƒ¡ãƒ³ãƒˆã®è¾æ›¸ãƒªã‚¹ãƒˆ
        images: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
        about_html: ã€Œåº—ä¸»ã«ã¤ã„ã¦ã€ã®HTML
        config: è¨­å®šæƒ…å ±ã®è¾æ›¸
        store_history: åº—èˆ—å¤‰é·ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        menu_stats: ãƒ¡ãƒ‹ãƒ¥ãƒ¼é›†è¨ˆçµæœã®è¾æ›¸ï¼ˆãƒ¡ãƒ‹ãƒ¥ãƒ¼å: å‡ºç¾å›æ•°ï¼‰
    """
    print(f"\nğŸ“ HTMLã‚’ç”Ÿæˆä¸­...")
    
    # Jinja2ç’°å¢ƒã‚’è¨­å®š
    env = Environment(
        loader=FileSystemLoader(TEMPLATES_DIR),
        autoescape=True
    )
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
    template = env.get_template("index.html")
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ¸¡ã™ãƒ‡ãƒ¼ã‚¿
    context = {
        "site_title": config.get("site", {}).get("title", "æƒ³ã„å‡ºã®ãƒ©ãƒ¼ãƒ¡ãƒ³ - ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«ã‚µã‚¤ãƒˆ"),
        "site_description": config.get("site", {}).get("description", "æ•…äººã‚’å²ã¶ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«ã‚µã‚¤ãƒˆ"),
        "shop_name": config.get("site", {}).get("shop_name", "æƒ³ã„å‡ºã®ãƒ©ãƒ¼ãƒ¡ãƒ³"),
        "hero": config.get("hero", {}),
        "navigation": config.get("navigation", {}),
        "sections": config.get("sections", {}),
        "footer": config.get("footer", {}),
        "ui": config.get("ui", {}),
        "config": config,
        "comments": comments,
        "images": images,
        "about_html": about_html,
        "store_history": store_history or [],
        "menu_stats": menu_stats or {},
        "generated_at": datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M"),
        "comment_count": len(comments),
        "image_count": len(images),
    }
    
    # HTMLã‚’ç”Ÿæˆ
    html_output = template.render(**context)
    
    # index.html ã‚’ public/ ã«å‡ºåŠ›
    output_path = PUBLIC_DIR / "index.html"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(html_output)
    print(f"âœ“ HTMLã‚’å‡ºåŠ›: {output_path}")
    
    # static/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ public/ ã«ã‚³ãƒ”ãƒ¼
    copy_static_files()


def copy_static_files():
    """
    static/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹ã‚’ public/ ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚
    """
    import shutil
    
    # CSS ã‚’ã‚³ãƒ”ãƒ¼
    css_src = STATIC_DIR / "css"
    css_dst = PUBLIC_DIR / "static" / "css"
    if css_src.exists():
        css_dst.mkdir(parents=True, exist_ok=True)
        for css_file in css_src.glob("*.css"):
            shutil.copy2(css_file, css_dst / css_file.name)
            print(f"âœ“ CSSã‚’ã‚³ãƒ”ãƒ¼: {css_file.name}")
    
    # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼
    img_src = OUTPUT_IMAGES_DIR
    img_dst = PUBLIC_DIR / "static" / "images"
    if img_src.exists():
        img_dst.mkdir(parents=True, exist_ok=True)
        for img_file in img_src.iterdir():
            if img_file.is_file():
                shutil.copy2(img_file, img_dst / img_file.name)
        print(f"âœ“ ç”»åƒã‚’ã‚³ãƒ”ãƒ¼: {len(list(img_src.iterdir()))} ãƒ•ã‚¡ã‚¤ãƒ«")


# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================

def main():
    """
    ãƒ¡ã‚¤ãƒ³ã®ãƒ“ãƒ«ãƒ‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    parser = argparse.ArgumentParser(
        description="ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«ã‚µã‚¤ãƒˆé™çš„ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ"
    )
    parser.add_argument(
        "--csv-url",
        type=str,
        default=DEFAULT_CSV_URL,
        help="Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®CSVå…¬é–‹URLï¼ˆã‚³ãƒ¡ãƒ³ãƒˆæŠ•ç¨¿ãƒ•ã‚©ãƒ¼ãƒ ï¼‰"
    )
    parser.add_argument(
        "--photo-url",
        type=str,
        default=DEFAULT_PHOTO_URL,
        help="Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®CSVå…¬é–‹URLï¼ˆå†™çœŸæŠ•ç¨¿ãƒ•ã‚©ãƒ¼ãƒ ãƒ»ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
    )
    parser.add_argument(
        "--skip-fetch",
        action="store_true",
        help="CSVã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨"
    )
    parser.add_argument(
        "--skip-download",
        action="store_true",
        help="ç”»åƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—"
    )
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸœ ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«ã‚µã‚¤ãƒˆ ãƒ“ãƒ«ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # 1. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆã‚’ç¢ºèª
    ensure_directories()
    
    # 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    config = load_config()
    
    # 3. CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ï¼‰
    if args.skip_fetch:
        df = load_local_csv()
        print(f"\nğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨: {len(df)} ä»¶")
    else:
        # CSV URLãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if not args.csv_url:
            print("\nâš ï¸ ã‚¨ãƒ©ãƒ¼: CSV URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print("   ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§è¨­å®šã—ã¦ãã ã•ã„:")
            print("   1. ç’°å¢ƒå¤‰æ•°: export CSV_URL='https://docs.google.com/...'")
            print("   2. ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³: python build.py --csv-url 'https://docs.google.com/...'")
            print("   3. ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨: python build.py --skip-fetch")
            sys.exit(1)
        
        # ã‚³ãƒ¡ãƒ³ãƒˆã¨å†™çœŸæŠ•ç¨¿ã®ä¸¡æ–¹ã®CSVã‚’å–å¾—ã—ã¦ãƒãƒ¼ã‚¸
        df = fetch_and_merge_csv_data(args.csv_url, args.photo_url)
    
    # 4. CSVå†…ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if not args.skip_download:
        download_images_from_csv(df)
    
    # 5. ç”»åƒã‚’å‡¦ç†
    images = process_images()
    
    # 6. Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã¿
    about_html = load_markdown_content("about.md")
    
    # 7. åº—èˆ—å¤‰é·ã‚’æŠ½å‡º
    about_html, store_history = extract_store_history(about_html)
    
    # 8. ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    comments = prepare_comments_data(df)
    
    # 8-1. ãƒ¡ãƒ‹ãƒ¥ãƒ¼é›†è¨ˆ
    menu_stats = aggregate_menu_items(df)
    
    # 9. HTMLã‚’ç”Ÿæˆ
    generate_html(comments, images, about_html, config, store_history, menu_stats)
    
    print("\n" + "=" * 60)
    print("âœ¨ ãƒ“ãƒ«ãƒ‰å®Œäº†!")
    print(f"   ã‚³ãƒ¡ãƒ³ãƒˆ: {len(comments)} ä»¶")
    print(f"   ç”»åƒ: {len(images)} ä»¶")
    print(f"   å‡ºåŠ›å…ˆ: {PUBLIC_DIR}")
    print("=" * 60)


if __name__ == "__main__":
    main()

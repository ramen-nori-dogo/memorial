#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
build.py - ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«ã‚µã‚¤ãƒˆé™çš„ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’è¡Œã„ã¾ã™ï¼š
1. Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
2. raw_images/ å†…ã®ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºã—ã¦ static/images/ ã«å‡ºåŠ›
3. Jinja2 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ HTML ã‚’ç”Ÿæˆ

Usage:
    python build.py [--csv-url URL]
"""

import os
import sys
import argparse
from pathlib import Path
from datetime import datetime
import json

import pandas as pd
import markdown
import requests
from jinja2 import Environment, FileSystemLoader
from PIL import Image

# =============================================================================
# è¨­å®š
# =============================================================================

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
BASE_DIR = Path(__file__).parent.resolve()

# å„ç¨®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹
TEMPLATES_DIR = BASE_DIR / "templates"
CONTENT_DIR = BASE_DIR / "content"
DATA_DIR = BASE_DIR / "data"
RAW_IMAGES_DIR = BASE_DIR / "raw_images"
STATIC_DIR = BASE_DIR / "static"
OUTPUT_IMAGES_DIR = STATIC_DIR / "images"
PUBLIC_DIR = BASE_DIR / "public"
CONFIG_FILE = BASE_DIR / "config.json"

# ç”»åƒå‡¦ç†è¨­å®š
MAX_IMAGE_WIDTH = 1200  # æœ€å¤§å¹…ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰
MAX_IMAGE_HEIGHT = 800  # æœ€å¤§é«˜ã•ï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰
IMAGE_QUALITY = 85      # JPEG/WebPå“è³ªï¼ˆ1-100ï¼‰
OUTPUT_FORMAT = "webp"  # å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆwebp ã¾ãŸã¯ jpgï¼‰

# Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®å…¬é–‹CSV URL
# ç’°å¢ƒå¤‰æ•° CSV_URL ã§è¨­å®šã™ã‚‹ã‹ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•° --csv-url ã§æŒ‡å®šã—ã¦ãã ã•ã„
DEFAULT_CSV_URL = os.environ.get("CSV_URL", "")


# =============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# =============================================================================

def ensure_directories():
    """
    å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€ãªã‘ã‚Œã°ä½œæˆã—ã¾ã™ã€‚
    """
    directories = [DATA_DIR, OUTPUT_IMAGES_DIR, PUBLIC_DIR]
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
        print(f"âœ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª: {directory}")


def load_config() -> dict:
    """
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆconfig.jsonï¼‰ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
    
    Returns:
        dict: è¨­å®šæƒ…å ±ã®è¾æ›¸
    """
    if not CONFIG_FILE.exists():
        print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {CONFIG_FILE}")
        print("   ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
        return {}
    
    try:
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config
    except Exception as e:
        print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        print("   ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
        return {}


def fetch_csv_data(csv_url: str) -> pd.DataFrame:
    """
    Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        csv_url: CSVå½¢å¼ã§å…¬é–‹ã•ã‚ŒãŸã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URL
    
    Returns:
        pandas.DataFrame: å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿
    """
    print(f"\nğŸ“¥ CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­: {csv_url[:50]}...")
    
    try:
        # URLã‹ã‚‰CSVã‚’å–å¾—
        response = requests.get(csv_url, timeout=30)
        response.raise_for_status()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’UTF-8ã«è¨­å®š
        response.encoding = 'utf-8'
        
        # CSVã‚’DataFrameã«å¤‰æ›
        from io import StringIO
        df = pd.read_csv(StringIO(response.text), encoding='utf-8')
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ä¿å­˜
        cache_path = DATA_DIR / "comments.csv"
        df.to_csv(cache_path, index=False, encoding="utf-8")
        print(f"âœ“ CSVãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {cache_path}")
        print(f"  â†’ {len(df)} ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã¾ã—ãŸ")
        
        return df
        
    except requests.RequestException as e:
        print(f"âš ï¸ CSVã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ä½¿ç”¨
        cache_path = DATA_DIR / "comments.csv"
        if cache_path.exists():
            print(f"  â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™: {cache_path}")
            return pd.read_csv(cache_path, encoding='utf-8')
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚ãªã‘ã‚Œã°ç©ºã®DataFrameã‚’è¿”ã™
        print("  â†’ ç©ºã®DataFrameã‚’ä½¿ç”¨ã—ã¾ã™")
        return pd.DataFrame()


def load_local_csv() -> pd.DataFrame:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥CSVã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
    
    Returns:
        pandas.DataFrame: èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ç©ºã®DataFrameï¼‰
    """
    cache_path = DATA_DIR / "comments.csv"
    if cache_path.exists():
        return pd.read_csv(cache_path, encoding='utf-8')
    return pd.DataFrame()


def download_image_from_google_drive(url: str, output_path: Path) -> bool:
    """
    Google Driveã®URLã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    
    Args:
        url: Google Driveã®URL
        output_path: ä¿å­˜å…ˆã®ãƒ‘ã‚¹
    
    Returns:
        bool: æˆåŠŸã—ãŸã‚‰True
    """
    try:
        # Google Driveã®URLå½¢å¼ã‚’å¤‰æ›
        # https://drive.google.com/open?id=FILE_ID
        # ã¾ãŸã¯ https://drive.google.com/file/d/FILE_ID/view
        # â†’ https://drive.google.com/uc?export=download&id=FILE_ID
        
        file_id = None
        if "id=" in url:
            file_id = url.split("id=")[1].split("&")[0]
        elif "/d/" in url:
            file_id = url.split("/d/")[1].split("/")[0]
        
        if not file_id:
            print(f"  âœ— URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {url}")
            return False
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç”¨URLã‚’æ§‹ç¯‰
        download_url = f"https://drive.google.com/uc?export=download&id={file_id}"
        
        # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        response = requests.get(download_url, timeout=30)
        response.raise_for_status()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(output_path, "wb") as f:
            f.write(response.content)
        
        return True
        
    except Exception as e:
        print(f"  âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return False


def download_images_from_csv(df: pd.DataFrame) -> int:
    """
    CSVã«å«ã¾ã‚Œã‚‹Google Drive URLã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
    
    Args:
        df: ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®DataFrame
    
    Returns:
        int: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸç”»åƒã®æ•°
    """
    if df.empty:
        return 0
    
    print(f"\nğŸ“¥ CSVå†…ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    RAW_IMAGES_DIR.mkdir(parents=True, exist_ok=True)
    downloaded_count = 0
    
    for idx, row in df.iterrows():
        # 5åˆ—ç›®ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹4ï¼‰ãŒå†™çœŸURL
        if len(row) <= 4:
            continue
        
        photo_url = row.iloc[4]
        
        # URLãŒç©ºã§ãªã„å ´åˆ
        if pd.notna(photo_url) and str(photo_url).strip() and str(photo_url) != "nan":
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— + è¡Œç•ªå·ï¼‰
            timestamp = row.iloc[0] if len(row) > 0 else ""
            safe_timestamp = str(timestamp).replace("/", "").replace(":", "").replace(" ", "_")
            filename = f"photo_{safe_timestamp}_{idx}.jpg"
            output_path = RAW_IMAGES_DIR / filename
            
            # æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
            if output_path.exists():
                print(f"  âŠ™ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¢å­˜ï¼‰: {filename}")
                continue
            
            print(f"  â¬‡ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}")
            if download_image_from_google_drive(str(photo_url), output_path):
                print(f"  âœ“ ä¿å­˜å®Œäº†: {filename}")
                downloaded_count += 1
    
    if downloaded_count > 0:
        print(f"  â†’ {downloaded_count} ä»¶ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
    else:
        print(f"  â†’ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–°ã—ã„ç”»åƒã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return downloaded_count


def process_images() -> list:
    """
    raw_images/ å†…ã®ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºã—ã¦ static/images/ ã«å‡ºåŠ›ã—ã¾ã™ã€‚
    
    Returns:
        list: å‡¦ç†ã•ã‚ŒãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
    """
    print(f"\nğŸ–¼ï¸ ç”»åƒã‚’å‡¦ç†ä¸­...")
    
    # å¯¾å¿œã™ã‚‹ç”»åƒå½¢å¼
    supported_extensions = {".jpg", ".jpeg", ".png", ".gif", ".webp", ".bmp"}
    processed_images = []
    
    if not RAW_IMAGES_DIR.exists():
        print(f"  â†’ raw_images/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return processed_images
    
    # raw_images/ å†…ã®ã™ã¹ã¦ã®ç”»åƒã‚’å‡¦ç†
    for image_path in RAW_IMAGES_DIR.iterdir():
        if image_path.suffix.lower() not in supported_extensions:
            continue
        
        try:
            # ç”»åƒã‚’é–‹ã
            with Image.open(image_path) as img:
                # RGBAã®å ´åˆã¯RGBã«å¤‰æ›ï¼ˆWebP/JPEGç”¨ï¼‰
                if img.mode in ("RGBA", "P"):
                    img = img.convert("RGB")
                
                # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¶­æŒã—ã¦ãƒªã‚µã‚¤ã‚º
                img.thumbnail((MAX_IMAGE_WIDTH, MAX_IMAGE_HEIGHT), Image.Resampling.LANCZOS)
                
                # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
                output_filename = f"{image_path.stem}.{OUTPUT_FORMAT}"
                output_path = OUTPUT_IMAGES_DIR / output_filename
                
                # ä¿å­˜
                if OUTPUT_FORMAT == "webp":
                    img.save(output_path, "WEBP", quality=IMAGE_QUALITY)
                else:
                    img.save(output_path, "JPEG", quality=IMAGE_QUALITY)
                
                processed_images.append(output_filename)
                print(f"  âœ“ {image_path.name} â†’ {output_filename}")
                
        except Exception as e:
            print(f"  âœ— {image_path.name} ã®å‡¦ç†ã«å¤±æ•—: {e}")
    
    print(f"  â†’ {len(processed_images)} ä»¶ã®ç”»åƒã‚’å‡¦ç†ã—ã¾ã—ãŸ")
    return sorted(processed_images)


def load_markdown_content(filename: str) -> str:
    """
    Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§HTMLã«å¤‰æ›ã—ã¾ã™ã€‚
    
    Args:
        filename: content/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«å
    
    Returns:
        str: HTMLã«å¤‰æ›ã•ã‚ŒãŸå†…å®¹
    """
    filepath = CONTENT_DIR / filename
    
    if not filepath.exists():
        print(f"âš ï¸ {filepath} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return ""
    
    with open(filepath, "r", encoding="utf-8") as f:
        md_content = f.read()
    
    # Markdownã‚’HTMLã«å¤‰æ›ï¼ˆæ‹¡å¼µæ©Ÿèƒ½ä»˜ãï¼‰
    html_content = markdown.markdown(
        md_content,
        extensions=["extra", "nl2br", "sane_lists"]
    )
    
    return html_content


def extract_store_history(about_html: str) -> tuple:
    """
    åº—èˆ—ã®å¤‰é·æƒ…å ±ã‚’HTMLã‹ã‚‰æŠ½å‡ºã—ã¾ã™ã€‚
    
    Args:
        about_html: about.mdã‹ã‚‰å¤‰æ›ã•ã‚ŒãŸHTML
    
    Returns:
        tuple: (åº—èˆ—å¤‰é·ã‚’é™¤ã„ãŸHTML, åº—èˆ—å¤‰é·ã®ãƒªã‚¹ãƒˆ)
    """
    import re
    
    # ã€Œåº—èˆ—ã®å¤‰é·ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
    history_pattern = r'<h2>åº—èˆ—ã®å¤‰é·</h2>(.*?)<hr\s*/>'
    match = re.search(history_pattern, about_html, re.DOTALL)
    
    if not match:
        return about_html, []
    
    history_section = match.group(1)
    about_without_history = about_html.replace(match.group(0), '<hr />')
    
    # å„åº—èˆ—ã®æƒ…å ±ã‚’æŠ½å‡ºï¼ˆh3ã¨imgã‚’åˆ¥ã€…ã«ï¼‰
    stores = []
    
    # h3ã‚¿ã‚°ã‚’æ¢ã™
    h3_pattern = r'<h3>(.*?)</h3>'
    img_pattern = r'<img\s+alt="(.*?)"\s+src="(.*?)"\s*/>'
    
    h3_matches = list(re.finditer(h3_pattern, history_section))
    img_matches = list(re.finditer(img_pattern, history_section))
    
    # h3ã¨imgã‚’é †ç•ªã«ãƒãƒƒãƒãƒ³ã‚°
    for idx, (h3_match, img_match) in enumerate(zip(h3_matches, img_matches)):
        title = h3_match.group(1)
        alt_text = img_match.group(1)
        image_url = img_match.group(2)
        
        stores.append({
            'id': f'store{idx}',
            'title': title,
            'alt': alt_text,
            'image': image_url
        })
    
    return about_without_history, stores


def prepare_comments_data(df: pd.DataFrame) -> list:
    """
    DataFrameã‚’ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”¨ã®è¾æ›¸ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚
    
    Args:
        df: ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®DataFrame
    
    Returns:
        list: ã‚³ãƒ¡ãƒ³ãƒˆã®è¾æ›¸ãƒªã‚¹ãƒˆ
    """
    comments = []
    
    # ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    if df.empty:
        return comments
    
    # Googleãƒ•ã‚©ãƒ¼ãƒ ã®åˆ—ã¯ä»¥ä¸‹ã®é †åº:
    # 0: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    # 1: åº—ä¸»æ§˜ã‚„ãƒ©ãƒ¼ãƒ¡ãƒ³NORIã«ã¾ã¤ã‚ã‚‹æ€ã„å‡ºã‚’æ•™ãˆã¦ä¸‹ã•ã„ï¼ˆå¿…é ˆï¼‰
    # 2: å…¬é–‹å¯èƒ½ãªãŠåå‰ï¼ˆãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã€ä»»æ„ï¼‰
    # 3: å¥½ãã ã£ãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’æ•™ãˆã¦ä¸‹ã•ã„ï¼ˆè¤‡æ•°å¯ã€ä»»æ„ï¼‰
    # 4: æ€ã„å‡ºã®å†™çœŸï¼ˆ1æš/1MBã¾ã§ã€ä»»æ„ï¼‰
    
    for idx, row in df.iterrows():
        # åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹
        timestamp = row.iloc[0] if len(row) > 0 else ""
        content = row.iloc[1] if len(row) > 1 else ""
        name = row.iloc[2] if len(row) > 2 else "åŒ¿å"
        menu = row.iloc[3] if len(row) > 3 else ""
        photo_url = row.iloc[4] if len(row) > 4 else ""
        
        # å†™çœŸã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚’ç‰¹å®šï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ç”»åƒï¼‰
        photo_filename = None
        if pd.notna(photo_url) and str(photo_url).strip() and str(photo_url) != "nan":
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            safe_timestamp = str(timestamp).replace("/", "").replace(":", "").replace(" ", "_")
            photo_filename = f"photo_{safe_timestamp}_{idx}.webp"
        
        comment = {
            "timestamp": timestamp,
            "content": str(content),
            "menu": str(menu),
            "photo_url": str(photo_url),
            "photo_filename": photo_filename,  # ãƒ­ãƒ¼ã‚«ãƒ«ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
            "name": str(name)
        }
        
        # åå‰ãŒç©ºã®å ´åˆã¯ã€ŒåŒ¿åã€ã«
        if pd.isna(comment["name"]) or comment["name"].strip() == "" or comment["name"] == "nan":
            comment["name"] = "åŒ¿å"
        
        # ãƒ¡ãƒ‹ãƒ¥ãƒ¼æƒ…å ±ã‚’ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«è¿½åŠ 
        if comment.get("menu") and not pd.isna(comment["menu"]) and comment["menu"].strip() and comment["menu"] != "nan":
            menu_text = f"\n\nã€å¥½ãã ã£ãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€‘\n{comment['menu']}"
            comment["content"] = comment["content"] + menu_text
        
        # ã‚³ãƒ¡ãƒ³ãƒˆãŒç©ºã§ãªã„å ´åˆã®ã¿è¿½åŠ 
        if not pd.isna(comment["content"]) and comment["content"].strip() and comment["content"] != "nan":
            comments.append(comment)
    
    # æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆï¼ˆTimestampãŒã‚ã‚‹å ´åˆï¼‰
    if comments and comments[0].get("timestamp"):
        try:
            comments.sort(key=lambda x: x["timestamp"], reverse=True)
        except:
            pass
    
    return comments


def generate_html(comments: list, images: list, about_html: str, config: dict, store_history: list = None):
    """
    Jinja2ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦HTMLã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    
    Args:
        comments: ã‚³ãƒ¡ãƒ³ãƒˆã®è¾æ›¸ãƒªã‚¹ãƒˆ
        images: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒªã‚¹ãƒˆ
        about_html: ã€Œåº—ä¸»ã«ã¤ã„ã¦ã€ã®HTML
        config: è¨­å®šæƒ…å ±ã®è¾æ›¸
        store_history: åº—èˆ—å¤‰é·ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    print(f"\nğŸ“ HTMLã‚’ç”Ÿæˆä¸­...")
    
    # Jinja2ç’°å¢ƒã‚’è¨­å®š
    env = Environment(
        loader=FileSystemLoader(TEMPLATES_DIR),
        autoescape=True
    )
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
    template = env.get_template("index.html")
    
    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æ¸¡ã™ãƒ‡ãƒ¼ã‚¿
    context = {
        "site_title": config.get("site", {}).get("title", "æƒ³ã„å‡ºã®ãƒ©ãƒ¼ãƒ¡ãƒ³ - ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«ã‚µã‚¤ãƒˆ"),
        "site_description": config.get("site", {}).get("description", "æ•…äººã‚’å²ã¶ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«ã‚µã‚¤ãƒˆ"),
        "shop_name": config.get("site", {}).get("shop_name", "æƒ³ã„å‡ºã®ãƒ©ãƒ¼ãƒ¡ãƒ³"),
        "hero": config.get("hero", {}),
        "navigation": config.get("navigation", {}),
        "sections": config.get("sections", {}),
        "footer": config.get("footer", {}),
        "ui": config.get("ui", {}),
        "comments": comments,
        "images": images,
        "about_html": about_html,
        "store_history": store_history or [],
        "generated_at": datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M"),
        "comment_count": len(comments),
        "image_count": len(images),
    }
    
    # HTMLã‚’ç”Ÿæˆ
    html_output = template.render(**context)
    
    # index.html ã‚’ public/ ã«å‡ºåŠ›
    output_path = PUBLIC_DIR / "index.html"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(html_output)
    print(f"âœ“ HTMLã‚’å‡ºåŠ›: {output_path}")
    
    # static/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ public/ ã«ã‚³ãƒ”ãƒ¼
    copy_static_files()


def copy_static_files():
    """
    static/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å†…å®¹ã‚’ public/ ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã™ã€‚
    """
    import shutil
    
    # CSS ã‚’ã‚³ãƒ”ãƒ¼
    css_src = STATIC_DIR / "css"
    css_dst = PUBLIC_DIR / "static" / "css"
    if css_src.exists():
        css_dst.mkdir(parents=True, exist_ok=True)
        for css_file in css_src.glob("*.css"):
            shutil.copy2(css_file, css_dst / css_file.name)
            print(f"âœ“ CSSã‚’ã‚³ãƒ”ãƒ¼: {css_file.name}")
    
    # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼
    img_src = OUTPUT_IMAGES_DIR
    img_dst = PUBLIC_DIR / "static" / "images"
    if img_src.exists():
        img_dst.mkdir(parents=True, exist_ok=True)
        for img_file in img_src.iterdir():
            if img_file.is_file():
                shutil.copy2(img_file, img_dst / img_file.name)
        print(f"âœ“ ç”»åƒã‚’ã‚³ãƒ”ãƒ¼: {len(list(img_src.iterdir()))} ãƒ•ã‚¡ã‚¤ãƒ«")


# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================

def main():
    """
    ãƒ¡ã‚¤ãƒ³ã®ãƒ“ãƒ«ãƒ‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    """
    parser = argparse.ArgumentParser(
        description="ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«ã‚µã‚¤ãƒˆé™çš„ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ"
    )
    parser.add_argument(
        "--csv-url",
        type=str,
        default=DEFAULT_CSV_URL,
        help="Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®CSVå…¬é–‹URL"
    )
    parser.add_argument(
        "--skip-fetch",
        action="store_true",
        help="CSVã®å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨"
    )
    parser.add_argument(
        "--skip-download",
        action="store_true",
        help="ç”»åƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—"
    )
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸœ ãƒ¡ãƒ¢ãƒªã‚¢ãƒ«ã‚µã‚¤ãƒˆ ãƒ“ãƒ«ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # 1. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆã‚’ç¢ºèª
    ensure_directories()
    
    # 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    config = load_config()
    
    # 3. CSVãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ï¼‰
    if args.skip_fetch:
        df = load_local_csv()
        print(f"\nğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨: {len(df)} ä»¶")
    else:
        # CSV URLãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if not args.csv_url:
            print("\nâš ï¸ ã‚¨ãƒ©ãƒ¼: CSV URLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print("   ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã§è¨­å®šã—ã¦ãã ã•ã„:")
            print("   1. ç’°å¢ƒå¤‰æ•°: export CSV_URL='https://docs.google.com/...'")
            print("   2. ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³: python build.py --csv-url 'https://docs.google.com/...'")
            print("   3. ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨: python build.py --skip-fetch")
            sys.exit(1)
        
        df = fetch_csv_data(args.csv_url)
    
    # 4. CSVå†…ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if not args.skip_download:
        download_images_from_csv(df)
    
    # 5. ç”»åƒã‚’å‡¦ç†
    images = process_images()
    
    # 6. Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã¿
    about_html = load_markdown_content("about.md")
    
    # 7. åº—èˆ—å¤‰é·ã‚’æŠ½å‡º
    about_html, store_history = extract_store_history(about_html)
    
    # 8. ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    comments = prepare_comments_data(df)
    
    # 9. HTMLã‚’ç”Ÿæˆ
    generate_html(comments, images, about_html, config, store_history)
    
    print("\n" + "=" * 60)
    print("âœ¨ ãƒ“ãƒ«ãƒ‰å®Œäº†!")
    print(f"   ã‚³ãƒ¡ãƒ³ãƒˆ: {len(comments)} ä»¶")
    print(f"   ç”»åƒ: {len(images)} ä»¶")
    print(f"   å‡ºåŠ›å…ˆ: {PUBLIC_DIR}")
    print("=" * 60)


if __name__ == "__main__":
    main()
